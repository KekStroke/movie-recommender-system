{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/internal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cpu'\n",
    "    epochs = 20\n",
    "    seed = 0\n",
    "    batch_size = 128\n",
    "    embedding_dim = 32\n",
    "    hidden_size = 32\n",
    "    lr = 1e-3\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=0):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_DIR, 'data_train.csv'))\n",
    "val_data = pd.read_csv(os.path.join(DATA_DIR, 'data_val.csv'))\n",
    "user_data = pd.read_csv(os.path.join(DATA_DIR, 'user.csv'), index_col=0)\n",
    "item_data = pd.read_csv(os.path.join(DATA_DIR, 'item.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 19)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_user_features = user_data.shape[1]\n",
    "n_item_features = item_data.shape[1]\n",
    "n_user_features, n_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, ratings, users, items):\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, ids):\n",
    "        ratings = self.ratings.iloc[ids]\n",
    "\n",
    "        user_ids = ratings.user_id.astype('int')\n",
    "        item_ids = ratings.item_id.astype('int')\n",
    "\n",
    "        users = self.users.iloc[user_ids]\n",
    "        items = self.items.iloc[item_ids]\n",
    "\n",
    "        return {\n",
    "            \"ratings\": torch.tensor(ratings.rating, dtype=torch.long),\n",
    "            \"user_ids\": torch.tensor(user_ids, dtype=torch.long),\n",
    "            \"item_ids\": torch.tensor(item_ids, dtype=torch.long),\n",
    "            \"users_info\": torch.tensor(users.to_numpy(), dtype=torch.float),\n",
    "            \"items_info\": torch.tensor(items.to_numpy(), dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieDataset(train_data, user_data, item_data)\n",
    "val_dataset = MovieDataset(val_data, user_data, item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 22])\n",
      "torch.Size([128, 19])\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data['ratings'].shape)\n",
    "    print(data['user_ids'].shape)\n",
    "    print(data['item_ids'].shape)\n",
    "    print(data['users_info'].shape)\n",
    "    print(data['items_info'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embed = nn.Embedding(n_users, embedding_dim=embedding_dim)\n",
    "        self.item_embed = nn.Embedding(n_items, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2 + n_item_features + n_user_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids, users_info, items_info):\n",
    "        user_embeds = self.user_embed(user_ids)\n",
    "        item_embeds = self.item_embed(item_ids)\n",
    "\n",
    "        x = torch.cat([user_embeds, item_embeds, users_info, items_info], dim=1)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSysModel(n_items=len(item_data), n_users=len(\n",
    "    user_data), embedding_dim=config.embedding_dim, hidden_size=config.hidden_size).to(config.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, criterion, test_dataloader: DataLoader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Function that evaluates model on specified dataloader\n",
    "    by specified loss function.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "      Model to train.\n",
    "    criterion\n",
    "      The loss function from pytorch\n",
    "    test_dataloader: DataLoader\n",
    "      The dataset for testing model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: loss of model on given dataset\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Test loss value\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            \n",
    "            user_ids = data['user_ids'].to(device)\n",
    "            item_ids = data['item_ids'].to(device)\n",
    "            users_info = data['users_info'].to(device)\n",
    "            items_info = data['items_info'].to(device)\n",
    "            ratings = data['ratings'].to(device)\n",
    "            \n",
    "            ratings = ratings.view(-1, 1).to(torch.float)            \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(user_ids=user_ids, item_ids=item_ids, users_info=users_info, items_info=items_info)\n",
    "            test_loss += criterion(outputs, ratings)\n",
    "\n",
    "    # Computation of test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "\n",
    "    return test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, epochs: int, criterion, train_dataloader, validation_dataloader, load_ckpt: bool = False, load_ckpt_path: str or None = None, save_ckpt_path: str = 'best.pt', device: torch.device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Function that trains model using number of epochs, loss function, optimizer.\n",
    "    Can use validation or test data set for evaluation.\n",
    "    Calculates f1 score.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "        Model to train.\n",
    "    epochs: int\n",
    "        Number of train epochs\n",
    "    criterion\n",
    "        The loss function from pytorch\n",
    "    train_dataloader\n",
    "        Dataloader of the train dataset\n",
    "    train_dataloader\n",
    "        Dataloader of the validation dataset\n",
    "    load_ckpt: bool\n",
    "        load model from checkpoint if true or train from scratch if false\n",
    "    load_ckpt_path: str\n",
    "        Path of already existing checkpoint to load model from\n",
    "    save_ckpt_path: str\n",
    "        Path of where to store the best model checkpoint\n",
    "    device: torch.device\n",
    "        Pytroch device\n",
    "    \"\"\"\n",
    "\n",
    "    if load_ckpt_path is None:\n",
    "        load_ckpt_path = save_ckpt_path\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # best score for checkpointing\n",
    "    best = 1000000000.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    first_epoch = 1\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    isCkptExists = os.path.isfile(load_ckpt_path)\n",
    "\n",
    "    if (load_ckpt and not isCkptExists):\n",
    "        print('Checkpoint file does not exist. Training model from scratch!')\n",
    "\n",
    "    if (load_ckpt and isCkptExists):\n",
    "        checkpoint = torch.load(load_ckpt_path)\n",
    "        best = checkpoint['best_score']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        train_losses = checkpoint['train_losses']\n",
    "        first_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(first_epoch, epochs + first_epoch):\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader)\n",
    "        iterations = 0\n",
    "\n",
    "        for data in bar:\n",
    "\n",
    "            user_ids = data['user_ids'].to(device)\n",
    "            item_ids = data['item_ids'].to(device)\n",
    "            users_info = data['users_info'].to(device)\n",
    "            items_info = data['items_info'].to(device)\n",
    "            ratings = data['ratings'].to(device)\n",
    "            \n",
    "            ratings = ratings.view(-1, 1).to(torch.float)            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(user_ids=user_ids, item_ids=item_ids, users_info=users_info, items_info=items_info)\n",
    "            loss = criterion(outputs, ratings)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iterations += 1\n",
    "            bar.set_postfix(\n",
    "                ({\"loss\": f\"{train_loss/(iterations*train_dataloader.batch_size)}\"}))\n",
    "\n",
    "        # Computing loss\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "\n",
    "        # Printing information in the end of train loop\n",
    "        val_loss = test_model(model, criterion, validation_dataloader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best:\n",
    "            best = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_score': best,\n",
    "            }, save_ckpt_path)\n",
    "\n",
    "        print(f\"Epoch {epoch}: \\ntrain:\\t\\t(loss: {train_loss:.4f}) \\nvalidation:\\t(loss: {val_loss:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:17<00:00, 30.62it/s, loss=0.021048668559669192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "train:\t\t(loss: 0.0211) \n",
      "validation:\t(loss: 1.1788)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.87it/s, loss=0.008463003031260038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: \n",
      "train:\t\t(loss: 0.0085) \n",
      "validation:\t(loss: 1.0417)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.81it/s, loss=0.007578821201591085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: \n",
      "train:\t\t(loss: 0.0076) \n",
      "validation:\t(loss: 0.9764)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.49it/s, loss=0.007097773362750409] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: \n",
      "train:\t\t(loss: 0.0071) \n",
      "validation:\t(loss: 0.9420)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.39it/s, loss=0.006810437480230384] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: \n",
      "train:\t\t(loss: 0.0068) \n",
      "validation:\t(loss: 0.9189)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.53it/s, loss=0.006616681487695582] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: \n",
      "train:\t\t(loss: 0.0066) \n",
      "validation:\t(loss: 0.9137)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.91it/s, loss=0.006482206551729206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: \n",
      "train:\t\t(loss: 0.0065) \n",
      "validation:\t(loss: 0.9057)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.19it/s, loss=0.00637349854196668]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: \n",
      "train:\t\t(loss: 0.0064) \n",
      "validation:\t(loss: 0.8961)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 30.76it/s, loss=0.006280743651238352] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: \n",
      "train:\t\t(loss: 0.0063) \n",
      "validation:\t(loss: 0.9002)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.76it/s, loss=0.006202981312191863] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: \n",
      "train:\t\t(loss: 0.0062) \n",
      "validation:\t(loss: 0.8966)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 30.98it/s, loss=0.0061419250218775765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: \n",
      "train:\t\t(loss: 0.0061) \n",
      "validation:\t(loss: 0.8981)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.52it/s, loss=0.006071755331601962] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: \n",
      "train:\t\t(loss: 0.0061) \n",
      "validation:\t(loss: 0.8971)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.15it/s, loss=0.005995386254695383] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: \n",
      "train:\t\t(loss: 0.0060) \n",
      "validation:\t(loss: 0.9032)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.47it/s, loss=0.005937170921075532] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: \n",
      "train:\t\t(loss: 0.0059) \n",
      "validation:\t(loss: 0.8983)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.18it/s, loss=0.005875687514096356] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: \n",
      "train:\t\t(loss: 0.0059) \n",
      "validation:\t(loss: 0.9012)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.84it/s, loss=0.005817360370473444] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: \n",
      "train:\t\t(loss: 0.0058) \n",
      "validation:\t(loss: 0.9030)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.37it/s, loss=0.005755030774418622] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: \n",
      "train:\t\t(loss: 0.0058) \n",
      "validation:\t(loss: 0.9032)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:23<00:00, 22.51it/s, loss=0.005699188189787997] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: \n",
      "train:\t\t(loss: 0.0057) \n",
      "validation:\t(loss: 0.9060)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:33<00:00, 15.79it/s, loss=0.00563815895198591]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: \n",
      "train:\t\t(loss: 0.0056) \n",
      "validation:\t(loss: 0.9056)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:33<00:00, 15.49it/s, loss=0.005584300182567075] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: \n",
      "train:\t\t(loss: 0.0056) \n",
      "validation:\t(loss: 0.9135)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model=model, epochs=config.epochs, criterion=loss_fn, train_dataloader=train_dataloader, validation_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(config.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_rows(a, indx, num_dups=1):\n",
    "    return np.insert(a,[indx+1]*num_dups,a[indx],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_movies(model, user_id, k=10):\n",
    "    model.eval()\n",
    "\n",
    "    watched_movies = val_data[val_data.user_id == user_id].item_id\n",
    "\n",
    "    unwatched_movies = item_data[~item_data.index.isin(watched_movies)]\n",
    "\n",
    "    unwatched_movie_ids = unwatched_movies.index.to_list()\n",
    "\n",
    "    users_info_one_row = user_data[user_data.index == user_id]\n",
    "    users_info = dup_rows(users_info_one_row.to_numpy(),\n",
    "                          0, len(unwatched_movie_ids)-1)\n",
    "\n",
    "    user_ids = [user_id] * len(unwatched_movie_ids)\n",
    "\n",
    "    items_info = np.zeros((len(unwatched_movie_ids), n_item_features))\n",
    "    for i, movie_id in enumerate(unwatched_movie_ids):\n",
    "        items_info[i] = item_data[item_data.index == movie_id].to_numpy()\n",
    "\n",
    "    user_ids = torch.tensor(user_ids).to(dtype=torch.long)\n",
    "    item_ids = torch.tensor(unwatched_movie_ids).to(dtype=torch.long)\n",
    "    users_info = torch.tensor(users_info).to(dtype=torch.float)\n",
    "    items_info = torch.tensor(items_info).to(dtype=torch.float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(user_ids=user_ids, item_ids=item_ids,\n",
    "                       users_info=users_info, items_info=items_info).squeeze()\n",
    "\n",
    "    unwatched_movies_with_rating = list(\n",
    "        zip(unwatched_movie_ids, output.numpy()))\n",
    "\n",
    "    top_k = sorted(unwatched_movies_with_rating,\n",
    "                   key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(349, 4.719418),\n",
       " (98, 4.6398897),\n",
       " (56, 4.637362),\n",
       " (10, 4.592761),\n",
       " (38, 4.547897),\n",
       " (152, 4.503314),\n",
       " (428, 4.4787703),\n",
       " (401, 4.469755),\n",
       " (166, 4.4374886),\n",
       " (669, 4.4250584)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(165, 4.8964577),\n",
       " (56, 4.8113103),\n",
       " (161, 4.774931),\n",
       " (10, 4.734867),\n",
       " (428, 4.7292366),\n",
       " (224, 4.696537),\n",
       " (109, 4.6899176),\n",
       " (76, 4.656498),\n",
       " (241, 4.65567),\n",
       " (430, 4.644674)]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(486, 4.1330075),\n",
       " (521, 4.0649877),\n",
       " (109, 3.9641457),\n",
       " (164, 3.928926),\n",
       " (424, 3.9173942),\n",
       " (56, 3.9163256),\n",
       " (426, 3.908978),\n",
       " (20, 3.894853),\n",
       " (143, 3.8934903),\n",
       " (698, 3.8702784)]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(349, 4.4354954),\n",
       " (56, 4.2890596),\n",
       " (144, 4.2192116),\n",
       " (669, 4.200879),\n",
       " (12, 4.1977253),\n",
       " (283, 4.1760993),\n",
       " (109, 4.173289),\n",
       " (428, 4.16583),\n",
       " (73, 4.148289),\n",
       " (143, 4.1428227)]"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(283, 4.397307),\n",
       " (432, 4.2847853),\n",
       " (279, 4.28288),\n",
       " (146, 4.276494),\n",
       " (397, 4.256823),\n",
       " (669, 4.248531),\n",
       " (487, 4.244829),\n",
       " (486, 4.227861),\n",
       " (42, 4.210864),\n",
       " (426, 4.172885)]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(349, 5.272317),\n",
       " (56, 5.25202),\n",
       " (556, 5.1639433),\n",
       " (130, 5.138515),\n",
       " (12, 5.1369667),\n",
       " (442, 5.1151314),\n",
       " (163, 5.0343633),\n",
       " (439, 5.030911),\n",
       " (386, 5.0135493),\n",
       " (73, 4.985412)]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_movies(model, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
